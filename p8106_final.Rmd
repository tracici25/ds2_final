---
title: "Final" 
author: "Yuechen Liu, Mufeng Xu, Yanhao Li"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


\newpage

```{r}
library(tidyverse)
library(caret)
library(glmnet)
library(ISLR)
library(pls)
library(AppliedPredictiveModeling)
library(MASS)
library(e1071)
library(mlbench)
library(pROC)
library(arsenal)
library(visdat)
library(pdp)
library(vip)
library(randomForest)
library(ranger)
library(gbm)
library(e1071)
library(kernlab)
```

# Introduction

Stroke is a serious life-threatening medical condition. According to the World Health Organization, stroke is the second leading cause of death globally. To better understand which factors correlate to the stroke event, our group find a stroke prediction dataset. This dataset contains twelve columns. The first column labels the unique identifier of the patient. The last column records the occurrence of stroke by 1 (Yes) or 0 (No). The other ten columns contain the observations of possible predictors.

# Load, clean, and tidy data

```{r}
stroke = read_csv("./healthcare-dataset-stroke-data.csv") %>% 
  dplyr::select(-id, -Residence_type, -ever_married, -smoking_status, -work_type) %>% 
  mutate(
    bmi = as.numeric(bmi),
    gender = as.factor(gender),
    hypertension = as.factor(hypertension),
    heart_disease = as.factor(heart_disease),
    stroke = as.factor(stroke)
  )
  

stroke1 = stroke %>% 
  janitor::clean_names() %>% 
  na.omit() %>% 
  filter(
    bmi != "N/A",
    gender != "Other"
  ) %>%
  mutate(
    gender = recode(
      gender,
      "Male" = 0,
      "Female" = 1
    ),
    stroke = recode(
      stroke,
     "0" = "No",
     "1" = "Yes"
    )
  ) %>% 
  relocate(
    age, avg_glucose_level, bmi
  )
```

# Exploratory analysis/ visualization

```{r}
stats = tableby(stroke ~ gender + age + hypertension + heart_disease + avg_glucose_level + bmi, data = stroke)

summary(stats, text = TRUE) %>% knitr::kable()

featurePlot(x = stroke1[, 1:3], 
            y = stroke1$stroke,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density",
            pch = "|", 
            auto.key = list(columns = 2),
            font = 2)
```

# Models

```{r}
set.seed(1)

indextrain <- createDataPartition(y = stroke1$stroke,
                               p = 0.8,
                               list = FALSE)

train = stroke1[indextrain, ]

test = stroke1[-indextrain, ]

x <- stroke1[indextrain, -c(7)]

y <- stroke1$stroke[indextrain] ###train

x2 <- stroke1[-indextrain, -c(7)]

y2 <- stroke1$stroke[-indextrain] ###test

ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
```

## GLM
```{r}
model.glm = train(stroke ~ . , 
                  data = train,
                  method = 'glm',
                  metric = "ROC",
                  trControl = ctrl)

glm.pred.prob = predict(model.glm, newdata = x2, type = "prob")[,1]

glm.pred = rep("No", length(glm.pred.prob))

glm.pred[glm.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(glm.pred),
                reference = y2,
                positive = "Yes")

model.glm$bestTune
```

## GLMN
```{r}
glmn.grid <- expand.grid(.alpha = seq(0, 1, length = 6),
                        .lambda = exp(seq(-8, -2, length = 20)))

set.seed(1)

model.glmn <- train(x = data.matrix(x),
                    y = y,
                    method = "glmnet",
                    tuneGrid = glmn.grid,
                    metric = "ROC",
                    trControl = ctrl)

glmn.pred.prob = predict(model.glmn, newdata = data.matrix(x2), type = "prob")[,1]

glmn.pred = rep("No", length(glmn.pred.prob))

glmn.pred[glmn.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(glmn.pred),
                reference = y2,
                positive = "Yes")

model.glmn$bestTune
```

## MARS
```{r}
set.seed(1)

model.mars <- train(stroke ~ . , 
                    data = train,
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3, 
                                           nprune = 2:15),
                    metric = "ROC",
                    trControl = ctrl)

mars.pred.prob = predict(model.mars, newdata = x2, type = "prob")[,1]

mars.pred = rep("No", length(mars.pred.prob))

mars.pred[mars.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(mars.pred),
                reference = y2,
                positive = "Yes")

model.mars$bestTune
```

## GAM
```{r}
set.seed(1)

model.gam <- train(stroke ~ . , 
                   data = train,
                   method = "gam",
                   metric = "ROC",
                   trControl = ctrl)

gam.pred.prob = predict(model.gam, newdata = x2, type = "prob")[,1]

gam.pred = rep("No", length(gam.pred.prob))

gam.pred[gam.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(gam.pred),
                reference = y2,
                positive = "Yes")

model.gam$bestTune
```

## LDA (from the midterm, LDA is the best among LDA, QDA and KNN)
```{r}
set.seed(1)

model.lda = train(x = data.matrix(x),
                  y = y,
                  method = "lda",
                  metric = "ROC",
                  trControl = ctrl)

lda.pred.prob = predict(model.lda, newdata = data.matrix(x2), type = "prob")[,1]

lda.pred = rep("No", length(lda.pred.prob))

lda.pred[lda.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(lda.pred),
                reference = y2,
                positive = "Yes")

model.lda$bestTune
```

## Random Forest
```{r}
rf.grid <- expand.grid(mtry = 1:6,
                       splitrule = "gini",
                       min.node.size = seq(from = 2, to = 10, by = 2))

set.seed(1)

model.rf <- train(stroke ~ . , 
                  data = train,
                method = "ranger",
                tuneGrid = rf.grid,
                metric = "ROC",
                trControl = ctrl,
                importance = "permutation")

ggplot(model.rf, highlight = TRUE)

rf.pred.prob = predict(model.rf, newdata = x2, type = "prob")[,1]

rf.pred = rep("No", length(rf.pred.prob))

rf.pred[rf.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(rf.pred),
                reference = y2,
                positive = "Yes")

model.rf$bestTune

barplot(sort(ranger::importance(model.rf$finalModel), decreasing = FALSE),
        las = 2, horiz = TRUE, cex.names = 0.7,
        col = colorRampPalette(colors = c("cyan","blue"))(19))
```

## gbmA
```{r}
gbmA.grid <- expand.grid(n.trees = c(2000,3000,4000),
                         interaction.depth = 1:6,
                         shrinkage = c(0.001,0.003,0.005),
                         n.minobsinnode = 1)

set.seed(1)

model.gbma <- train(stroke ~ . , 
                  data = train,
                  tuneGrid = gbmA.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)

ggplot(model.gbma, highlight = TRUE)

test.pred.prob = predict(model.gbma, newdata = x2, type = "prob")[,1]

test.pred = rep("No", length(test.pred.prob))

test.pred[test.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(test.pred),
                reference = y2,
                positive = "Yes")

model.gbma$bestTune
```

## svml
```{r}
set.seed(1)

model.svml <- train(stroke ~ . , 
                  data = train, 
                  method = "svmLinear",
                  preProcess = c("center", "scale"),
                  tuneGrid = data.frame(C = exp(seq(0, 2,len = 20))),
                  metric = "ROC",
                  trControl = ctrl)

plot(model.svml, highlight = TRUE, xTrans = log)

svml.pred.prob = predict(model.svml, newdata = x2, type = "prob")[,1]

svml.pred = rep("No", length(svml.pred.prob))

svml.pred[svml.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(svml.pred),
                reference = y2,
                positive = "Yes")

model.svml$bestTune
```

## svmr
```{r}
svmr.grid <- expand.grid(C = exp(seq(-1,4,len = 10)),
                         sigma = exp(seq(-8,0,len = 10)))

set.seed(1)

model.svmr <- train(stroke ~ . , 
                  data = train,
                  method = "svmRadialSigma",
                  preProcess = c("center", "scale"),
                  tuneGrid = svmr.grid,
                  trControl = ctrl)

plot(model.svmr, highlight = TRUE)

svmr.pred.prob = predict(model.svmr, newdata = x2, type = "prob")[,1]

svmr.pred = rep("No", length(svmr.pred.prob))

svmr.pred[svmr.pred.prob < 0.9] = "Yes"

confusionMatrix(data = as.factor(svmr.pred),
                reference = y2,
                positive = "Yes")

model.svmr$bestTune
```

# Comparison

```{r}
res <- resamples(list(glm = model.glm, 
                      glmn = model.glmn, 
                      mars = model.mars, 
                      gam = model.gam,
                      lda = model.lda,
                      rf = model.rf,
                      gbmA = model.gbma,
                      svml = model.svml,
                      svmr = model.svmr))

summary(res)

bwplot(res, metric = "ROC")
```

